If this doesn't finish on the GCP instance:
    Load up the topic models
    Grab the first 100 (or 10) - based on number of documents in a cluster
    Summarize the text in just those docs instead of the whole data set

If I go the above route:
    Will need to make a mapping function that can take in a list of doc ids
    Use doc ids to retrieve the corresponding rows from the data frame
    Compute the summaries, append to the sub-dataframe
    Additionally, concatenate all summaries and compute a "topic summary"
    Probably store all of this in json. Can have a "csv" key that you can load into a dataframe.


Thoughts:
    Load Models
    Get top x documents from each topic
        Where x is a threshold close to the center
    Summarize those documents as the initial topic summary
    Create a concatenation of the x summaries
        Summarize those as the overall topic summary

    Again, see the notes above about mapping from the doc ids to the data frame